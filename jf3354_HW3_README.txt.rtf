{\rtf1\ansi\ansicpg936\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\partightenfactor0

\f0\fs22 \cf2 \expnd0\expndtw0\kerning0
README:\
Put all the documents in the same folder\
To run the program, in terminal:\
python3.6 jf3354_HMM+viterbi_HW3.py  WSJ_02-21.pos WSJ_23.words\
This will create an output file named submission.pos\
\
My algorithm is maintain a probability dict for the last word in order to determine the tag of the next word instead of maintaining the whole matrix and traversing the matrix.\
\
For OOV, I treat them as having the same small probability. Thus I think that the likelihood probability of OOV doesn\'92t matter when comparing all the probabilities of possible tags so I just ignore this part of the product when picking the max probability and picking that tag for the word.\
\
}